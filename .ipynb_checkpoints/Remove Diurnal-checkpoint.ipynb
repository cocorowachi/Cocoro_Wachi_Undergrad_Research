{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dc144a-93e0-4634-a8e3-20b03da9c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta, SU\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1786cd50-9ee3-409a-b4ce-62b8634a56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = \"C:/Users/wachic/OneDrive - Milwaukee School of Engineering/Documents/GitHub/Undergrad_Research/\"\n",
    "\n",
    "# Naming convention\n",
    "# MMSD_sewerflow_all_dailyavg_df\n",
    "# 1.  [MMSD, USGS] Where the source is\n",
    "# 2.  [sewerflow, precip, streamflow] What the data measures\n",
    "# 3.  [all, dry, wet] what season it includes\n",
    "# 4-n What ever operation has been done to the data\n",
    "# n+1 [df, periods, csv] data type\n",
    "\n",
    "USGS_stream_flow_all_df = pd.read_csv(working_directory + \"USGS 04087030 Streamflow Cleaned.csv\")\n",
    "MMSD_sewerflow_all_df = pd.read_csv(working_directory + \"MMSD Sewer Flow Cleaned.csv\")\n",
    "MMSD_flow_and_precip_all_df = pd.read_csv(working_directory + \"MMSD Flow and Precipitation Cleaned.csv\")\n",
    "MMSD_precip_all_df = pd.read_csv(working_directory + \"MMSD Precipitation Raw Data Cleaned.csv\")\n",
    "\n",
    "df_list = [USGS_stream_flow_all_df,\n",
    "           MMSD_sewerflow_all_df,\n",
    "           MMSD_flow_and_precip_all_df,\n",
    "           MMSD_precip_all_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00526b66-cf47-4885-a1da-fea3b96f2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Date Time'] = pd.to_datetime(df['Date Time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb768125-e2fa-46f6-ad1f-c1c3f918ddb6",
   "metadata": {},
   "source": [
    "# Removing Diurnal Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c2be36-8733-464f-97d7-0bf0d30876eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a total column, used primarily for precipitation\n",
    "def create_total_col(df):\n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    Adds on a 'total' column\n",
    "    Used for precipitaiton df when calculating dry period\n",
    "    \"\"\"\n",
    "    df['total'] = df.loc[:, df.columns[df.columns.get_loc('Date Time') + 1:]].sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79dfd537-8346-4ff1-bc31-d988ae01bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(df1, df2):\n",
    "    columns_to_subtract = df1.drop(columns='Date Time').columns\n",
    "\n",
    "    df_diff = df1.copy()  # Copy df1 to keep other columns\n",
    "    df_diff[columns_to_subtract] = df1[columns_to_subtract] - df2[columns_to_subtract]\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6840a22a-a5e2-4486-9844-a7d6f29a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_na(edit_df, with_na_df):\n",
    "    edit_df = edit_df.copy()\n",
    "    with_na_df = with_na_df.copy()\n",
    "    edit_df[with_na_df.isna()] = np.nan\n",
    "    return edit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca3e6a9-eb16-4b18-8996-4a448b7461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df1, df2):\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    \"\"\"\n",
    "    Standardize 2 df with same columns\n",
    "    Combines vertically, standardize, then separate and return\n",
    "    Returns the two seprate std df and 2 df with mean and with std.dev in each column\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df_to_standardize = df.drop(columns=['Date Time'], inplace=False)\n",
    "    standardized_values = scaler.fit_transform(df_to_standardize)\n",
    "\n",
    "    # Create standardized DataFrame\n",
    "    df = pd.concat([df['Date Time'], pd.DataFrame(standardized_values, columns=df_to_standardize.columns)], axis=1)\n",
    "\n",
    "    # Compute standard deviation of each column\n",
    "    mean_df = pd.DataFrame(df_to_standardize.mean(), columns=['Mean']).T\n",
    "    std_dev_df = pd.DataFrame(df_to_standardize.std(), columns=['Standard Deviation']).T\n",
    "\n",
    "    df.columns = df.columns\n",
    "    df1_out = df.iloc[:len(df1)].reset_index(drop=True)\n",
    "    df2_out = df.iloc[len(df1):].reset_index(drop=True)\n",
    "    \n",
    "    return df1_out, df2_out, std_dev_df, mean_df\n",
    "    \n",
    "def revert_standardize(df, std_dev_df, mean_df):\n",
    "    df = df.copy()\n",
    "    std_dev_df = std_dev_df.copy()\n",
    "    mean_df = mean_df.copy()\n",
    "    \"\"\"\n",
    "    From a standardized df and its means and std.dev, reverts back to normal scale values\n",
    "    \"\"\"\n",
    "    columns = df.drop(columns=['Date Time'], inplace=False).columns\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    # Apply the inverse transformation\n",
    "    df_original[columns] = (df_original[columns] * std_dev_df.loc['Standard Deviation', columns]) + mean_df.loc['Mean', columns]\n",
    "    return df_original\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9efe1e-cab0-434e-95d9-8f2c89244020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_dry_period(df, dry_length = 8, days_after = 7, precip_threshold = 0):  \n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    Finds dry period from precipitation df. \n",
    "    Can select the length of days there must be without rain to be \"dry\"(dry_length)\n",
    "    The grace period where data isnt used. Only days after this period is used (days_after)\n",
    "    and what amount of precipitation is considered a rainfall (precip_threshold)\n",
    "    \"\"\"\n",
    "    zero_ranges = []\n",
    "    in_zero_sequence = False\n",
    "    start_date = None\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['total'] <= precip_threshold:  \n",
    "            if not in_zero_sequence:\n",
    "                # Start of a new zero sequence\n",
    "                in_zero_sequence = True\n",
    "                start_date = row['Date Time']\n",
    "        else:  # Found a non-zero value\n",
    "            if in_zero_sequence:\n",
    "                # End of the zero sequence\n",
    "                end_date = row['Date Time'] - timedelta(days=1) \n",
    "                date_diff = (end_date - start_date).days\n",
    "                if date_diff >= dry_length:  \n",
    "                    zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "                in_zero_sequence = False\n",
    "                start_date = None\n",
    "    \n",
    "    # Handle case where the last sequence of zeros extends to the last row\n",
    "    if in_zero_sequence:\n",
    "        end_date = row['Date Time'] \n",
    "        date_diff = (end_date - start_date).days\n",
    "        if date_diff >= dry_length:\n",
    "            zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "    return zero_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e70c5f-6390-4ccf-8eda-d6a9efe69ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dry_period(df, dry_period):\n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    Selects rows that are within the given dry_period\n",
    "    \"\"\"\n",
    "    mask = reduce(lambda x, y: x | y, [(df['Date Time'].between(start, end)) for start, end in dry_period])\n",
    "    \n",
    "    filtered_df = df[mask].reset_index(drop=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d06fe8-9f16-4113-97eb-c41fbfb3877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_days_of_week(df):\n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    Calculates the average of each hour and day of week, and returns values. [Sun 0:00AM to Sat 23:00PM]\n",
    "    Subtracts min() from each column, so min is 0\n",
    "    \"\"\"\n",
    "    # Group by weekday (Sunday=0, Saturday=6) and hour\n",
    "    df['weekday'] = df['Date Time'].dt.weekday  # Monday=0, Sunday=6\n",
    "    df['hour'] = df['Date Time'].dt.hour\n",
    "    \n",
    "    # Adjust weekday order to start from Sunday (moving Sunday=6 to 0)\n",
    "    df['weekday'] = (df['weekday'] + 1) % 7  # Convert Monday=0 â†’ Sunday=0\n",
    "    \n",
    "    hourly_avg = df.groupby(['weekday', 'hour']).mean()\n",
    "    df = df.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg = hourly_avg.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg.columns = df.columns\n",
    "    \n",
    "    # # Sort the values correctly from Sunday 00:00 to Saturday 23:00\n",
    "    hourly_avg = hourly_avg.sort_values(by=['weekday', 'hour'])\n",
    "\n",
    "    # just a refernce datetime from Sun 0:00 to Sat 23:00\n",
    "    date_range = pd.date_range(start=\"2025-02-02 00:00\", end=\"2025-02-08 23:00\", freq=\"h\")\n",
    "    \n",
    "    hourly_avg['Date Time'] = date_range\n",
    "    \n",
    "    num_cols = [col for col in hourly_avg.columns if col != 'Date Time']\n",
    "    hourly_avg[num_cols] = hourly_avg[num_cols] - hourly_avg[num_cols].min()\n",
    "    \n",
    "    return hourly_avg[['Date Time'] + [col for col in hourly_avg.columns if col != 'Date Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a283fe39-f8ea-47ca-a3b0-cf066dfbb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_diurnal(orig_df, correction_df): \n",
    "    orig_df = orig_df.copy()\n",
    "    correction_df = correction_df.copy()\n",
    "    \"\"\"\n",
    "    For each datapoint in orig_df, it subtracts the corresponding value from correction_df that has the same hour and day of week from.     \n",
    "    \"\"\"\n",
    "    columns_to_rename = correction_df.drop(columns='Date Time').columns\n",
    "    correction_df.columns = [f'{col} new' if col in columns_to_rename else col for col in correction_df.columns]\n",
    "    \n",
    "    orig_df['weekday'] = orig_df['Date Time'].dt.weekday\n",
    "    correction_df['weekday'] = correction_df['Date Time'].dt.weekday\n",
    "    \n",
    "    orig_df['hour'] = orig_df['Date Time'].dt.hour\n",
    "    correction_df['hour'] = correction_df['Date Time'].dt.hour\n",
    "    \n",
    "    correction_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "\n",
    "    # Set index for main DataFrame to match correction DataFrame\n",
    "    orig_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "    \n",
    "    # Reindex correction DataFrame to match the index of main DataFrame\n",
    "    correction_df = correction_df.reindex(orig_df.index)\n",
    "    \n",
    "    # Concatenate both DataFrames to align corrections with the main DataFrame\n",
    "    removed_df = pd.concat([orig_df, correction_df], axis=1)\n",
    "    for col in orig_df.drop(columns='Date Time').columns:      \n",
    "        removed_df[col] = removed_df[col] - removed_df[f'{col} new']\n",
    "    \n",
    "    removed_df.reset_index(drop=True, inplace=True)\n",
    "    difference_df = removed_df.drop(columns=orig_df.columns)\n",
    "    removed_df = removed_df[orig_df.columns]\n",
    "    return removed_df.iloc[:, [i for i in range(removed_df.shape[1]) if i != 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5fb33-9eb2-4e8f-b74a-6cba640f6956",
   "metadata": {},
   "source": [
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2fb5285-0a06-4932-9977-8ae33bed3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(MMSD_sewerflow_all_df.dtypes)\n",
    "# df = MMSD_sewerflow_all_df.copy()\n",
    "# df.set_index('Date Time', inplace=True)\n",
    "# df = df.dropna(subset=['MS0311 Flow'])\n",
    "# # print(df.dtypes)\n",
    "# # Find index of max value for each day\n",
    "# idx_max = df.groupby(df.index.date)['MS0311 Flow'].idxmax()\n",
    "# idx_min = df.groupby(df.index.date)['MS0311 Flow'].idxmin()\n",
    "\n",
    "# # Create DataFrame with time and max value\n",
    "# daily_max_df = pd.DataFrame({\n",
    "#     'Date Time': [i.date() for i in idx_max],\n",
    "#     'time_of_max': idx_max.values,\n",
    "#     'Max': df.loc[idx_max, 'MS0311 Flow'].values,\n",
    "#     'Min': df.loc[idx_min, 'MS0311 Flow'].values\n",
    "# })\n",
    "# daily_max_df['hour_of_max'] = daily_max_df['time_of_max'].apply(lambda x: x.hour)\n",
    "# print(daily_max_df)\n",
    "# # test_csv = daily_max_df.to_csv('daily_max_df2.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39515783-d46f-4cfe-84e6-f2fa124c7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_df(time_series_df, df, num):\n",
    "    time_series_df = time_series_df.copy()\n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    repeats the daily avg df so it's easier to compare when aligned with other df\n",
    "    \"\"\"\n",
    "    extended_df = pd.concat([df] * num, ignore_index=True)\n",
    "    extended_df['Date Time'] = time_series_df['Date Time']\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4a2a9e-2f89-4f1e-a390-40e099519db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_diurnal(df, ref_df):\n",
    "    df = df.copy()\n",
    "    ref_df = ref_df.copy()\n",
    "    df.set_index('Date Time', inplace=True)\n",
    "    ref_df.set_index('Date Time', inplace=True)\n",
    "    ref_daily_max = ref_df['MS0311 Flow'].resample('D').max()*1.1\n",
    "\n",
    "    # Function to compare daily max threshold\n",
    "    def day_below_max(x):\n",
    "        day = x.index[0].normalize()\n",
    "        max_threshold = ref_daily_max.get(day, float('inf'))  # Default to inf if day is missing\n",
    "        return x['MS0311 Flow'].max() <= max_threshold\n",
    "\n",
    "    # Filter days where max is <= daily max from ref_df\n",
    "    days_with_min_0 = df.groupby(df.index.normalize()).filter(day_below_max)\n",
    "\n",
    "    # Reset index to bring back 'Date Time' as a column\n",
    "    days_with_min_0 = days_with_min_0.reset_index()\n",
    "\n",
    "    return days_with_min_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89de0305-f1a2-4ab5-86ac-e9117777f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mirrored_rows(df, num_rows=30):\n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    Insert chronologically mirrored data point at head and tail of df\n",
    "    \"\"\"\n",
    "    mirrored_rows_head = df.iloc[:num_rows].copy()\n",
    "    mirrored_rows_head = mirrored_rows_head.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    mirrored_rows_tail = df.iloc[-num_rows:].copy()\n",
    "    mirrored_rows_tail = mirrored_rows_tail.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    df_extended = pd.concat([mirrored_rows_head, df, mirrored_rows_tail], ignore_index=True)\n",
    "    \n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7b36fa2-b8af-4c7d-b5fc-3f2de4a06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(df, length=24):\n",
    "    df = df.copy()\n",
    "    \"\"\" \n",
    "    Finds moving average for past 24 hour\n",
    "    \"\"\"\n",
    "    columns_to_edit = df.drop(columns='Date Time').columns\n",
    "    out_df = df.copy()\n",
    "    for col in columns_to_edit:\n",
    "        out_df[col] = df[col].rolling(window=length).mean()\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849aac59-8d85-4d92-b017-d7f5f737a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noisy dataset\n",
    "def PCHIP(df, passes=1):\n",
    "    df = df.copy()\n",
    "    df.ffill(inplace=True)\n",
    "    timestamps = df['Date Time'].astype(np.int64) // 10**9  # to seconds\n",
    "    timestamps_linspace = np.linspace(timestamps.min(), timestamps.max(), len(df))\n",
    "    x = timestamps_linspace\n",
    "    \n",
    "    columns_to_work = df.drop(columns='Date Time').columns\n",
    "    for col in columns_to_work:\n",
    "        y = df[col].to_numpy()\n",
    "\n",
    "        # Identify local minima\n",
    "        local_minima_indices = argrelextrema(y, np.less)[0]\n",
    "\n",
    "        # Extract x and y values at local minima\n",
    "        x_minima = x[local_minima_indices]\n",
    "        y_minima = y[local_minima_indices]\n",
    "        \n",
    "        # Create a PCHIP interpolator\n",
    "        pchp_interpolator = PchipInterpolator(x_minima, y_minima)\n",
    "        \n",
    "        # Evaluate the interpolator on the original x values\n",
    "        y_smooth = pchp_interpolator(x)\n",
    "        df[col] = y_smooth\n",
    "    if passes > 1:\n",
    "        return PCHIP(df, passes-1)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def PCHIP_init(df, passes=1):\n",
    "    df = df.copy()\n",
    "    return PCHIP(insert_mirrored_rows(df), passes).iloc[30:-30].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622097f0-5cde-4ade-aaae-daa42a179916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyne_hollick(df, passes=1, alpha=0.925):\n",
    "    df = df.copy()\n",
    "    \"\"\"\n",
    "    Applies Lyne-Hollick filter recursively\n",
    "    Number of passes shall be odd\n",
    "    \"\"\"\n",
    "    copy_df = df.copy()\n",
    "    if passes < 1:\n",
    "        return df\n",
    "\n",
    "    if passes % 2 == 0:\n",
    "        copy_df = copy_df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    # Handle MultiIndex by flattening if necessary\n",
    "    if isinstance(copy_df.columns, pd.MultiIndex):\n",
    "        copy_df.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in copy_df.columns]\n",
    "    print(\"pass\")\n",
    "    \n",
    "    out_array = copy_df.copy().to_numpy(copy=True)\n",
    "    df_array = copy_df.to_numpy(copy=True)\n",
    "    for col_i in range(1,df.shape[1]): \n",
    "        for row_i in range(len(df)):\n",
    "            out_array[row_i, col_i] = df_array[row_i, col_i] - np.maximum(\n",
    "                alpha*(df_array[row_i-1,col_i] - out_array[row_i-1,col_i]) + ((1+alpha)/2)*(df_array[row_i,col_i] - df_array[row_i-1,col_i]),\n",
    "                0)\n",
    "            if pd.isna(out_array[row_i, col_i]):\n",
    "                out_array[row_i, col_i] = df_array[row_i, col_i]\n",
    "            elif out_array[row_i, col_i] <= 0: #need to make value dynamic\n",
    "                out_array[row_i, col_i] = out_array[row_i-1, col_i]\n",
    "    print(passes)\n",
    "    \n",
    "    out_df = pd.DataFrame(out_array, columns=df.columns)\n",
    "    # out_df.insert(0, 'Date Time', df['Date Time'])\n",
    "    \n",
    "    if passes % 2 == 0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    if passes > 1:\n",
    "        return lyne_hollick(out_df, passes-1, alpha)\n",
    "    else:\n",
    "        return out_df\n",
    "def lyne_hollick_init(df, passes=1,alpha=0.925):\n",
    "    df = df.copy()\n",
    "    return lyne_hollick(insert_mirrored_rows(df), passes, alpha).iloc[30:-30].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e23fd8b-0df4-4a65-a148-08587efa9991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "7\n",
      "pass\n",
      "6\n",
      "pass\n",
      "5\n",
      "pass\n",
      "4\n",
      "pass\n",
      "3\n",
      "pass\n",
      "2\n",
      "pass\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# actually using the methods\n",
    "USGS_stream_flow_all_df\n",
    "MMSD_sewerflow_all_df\n",
    "MMSD_flow_and_precip_all_df\n",
    "MMSD_precip_all_df\n",
    "\n",
    "USGS_stream_flow_all_fillna_df = USGS_stream_flow_all_df.copy().ffill()\n",
    "MMSD_sewerflow_all_fillna_df = MMSD_sewerflow_all_df.copy().ffill()\n",
    "MMSD_flow_and_precip_all_fillna_df = MMSD_flow_and_precip_all_df.copy().ffill()\n",
    "MMSD_precip_all_fillna_df = MMSD_precip_all_df.copy().ffill()\n",
    "\n",
    "# Create total precipitation level column to find dry season\n",
    "MMSD_precip_all_df = create_total_col(MMSD_precip_all_df)\n",
    "\n",
    "# Find dry period from precipitation data\n",
    "MMSD_precip_dry_periods = find_consecutive_dry_period(MMSD_precip_all_df, 10, 9, 0.0)\n",
    "\n",
    "# Extract dry season data from sewer flow to get dry season diurnal data\n",
    "MMSD_sewerflow_dry_df = select_dry_period(MMSD_sewerflow_all_df, MMSD_precip_dry_periods)\n",
    "\n",
    "# Get avg from [Sunday 0:00 to Saturday 23:00] of the dry season sewer flow\n",
    "MMSD_sewerflow_dry_dailyavg_df = avg_days_of_week(MMSD_sewerflow_dry_df)\n",
    "\n",
    "# Subtract the average from the full period sewer flow\n",
    "MMSD_sewerflow_all_removed_df = subtract_diurnal(MMSD_sewerflow_all_df, MMSD_sewerflow_dry_dailyavg_df)\n",
    "\n",
    "# Apply LH7\n",
    "MMSD_sewerflow_all_LH7_df = lyne_hollick_init(MMSD_sewerflow_all_df, 7, 0.925)\n",
    "\n",
    "MMSD_sewerflow_all_LH7_isolated_df = diff(MMSD_sewerflow_all_df, MMSD_sewerflow_all_LH7_df)\n",
    "\n",
    "MMSD_sewerflow_all_reference_diurnal_df = get_normal_diurnal(MMSD_sewerflow_all_LH7_isolated_df, MMSD_sewerflow_dry_dailyavg_df)\n",
    "\n",
    "MMSD_sewerflow_all_new_diurnal_df = avg_days_of_week(MMSD_sewerflow_all_reference_diurnal_df)\n",
    "\n",
    "MMSD_sewerflow_all_diurnal_removed_df = subtract_diurnal(MMSD_sewerflow_all_df, MMSD_sewerflow_all_new_diurnal_df)\n",
    "\n",
    "MMSD_sewerflow_all_diurnal_removed_withna_df = set_na(MMSD_sewerflow_all_diurnal_removed_df, MMSD_sewerflow_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e91ae8a-f1c8-4f51-b5d7-0d60ddcc0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMSD_sewerflow_all_diurnal_removed_withna_csv = MMSD_sewerflow_all_diurnal_removed_withna_df.to_csv('MMSD_sewerflow_all_diurnal_removed_withna_df.csv', index = False) \n",
    "MMSD_sewerflow_all_csv = MMSD_sewerflow_all_df.to_csv('MMSD_sewerflow_all_df.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51f35c6b-ef81-4f5d-8a29-4bc8c93f4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# have to use processed precip data instead of day cumulative -> find hourly cumulation instead of daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce79d5-ef85-4b1f-9542-62fcf68edde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
