{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dc144a-93e0-4634-a8e3-20b03da9c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta, SU\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1786cd50-9ee3-409a-b4ce-62b8634a56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = \"C:/Users/wachic/OneDrive - Milwaukee School of Engineering/Desktop/Undergrad Research/\"\n",
    "\n",
    "# Naming convention\n",
    "# MMSD_sewerflow_all_dailyavg_df\n",
    "# 1.  [MMSD, USGS] Where the source is\n",
    "# 2.  [sewerflow, precip, streamflow] What the data measures\n",
    "# 3.  [all, dry, wet] what season it includes\n",
    "# 4-n What ever operation has been done to the data\n",
    "# n+1 [df, periods, csv] data type\n",
    "\n",
    "USGS_stream_flow_all_df = pd.read_csv(working_directory + \"USGS 04087030 Streamflow Cleaned.csv\")\n",
    "MMSD_sewerflow_all_df = pd.read_csv(working_directory + \"MMSD Sewer Flow Cleaned.csv\")\n",
    "MMSD_flow_and_precip_all_df = pd.read_csv(working_directory + \"MMSD Flow and Precipitation Cleaned.csv\")\n",
    "MMSD_precip_all_df = pd.read_csv(working_directory + \"MMSD Precipitation Raw Data Cleaned.csv\")\n",
    "\n",
    "df_list = [USGS_stream_flow_all_df,\n",
    "           MMSD_sewerflow_all_df,\n",
    "           MMSD_flow_and_precip_all_df,\n",
    "           MMSD_precip_all_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00526b66-cf47-4885-a1da-fea3b96f2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Date Time'] = df['Date Time'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb768125-e2fa-46f6-ad1f-c1c3f918ddb6",
   "metadata": {},
   "source": [
    "# Removing Diurnal Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b36fa2-b8af-4c7d-b5fc-3f2de4a06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(df, length=24):\n",
    "    \"\"\" \n",
    "    Finds moving average for past 24 hour\n",
    "    \"\"\"\n",
    "    columns_to_edit = df.drop(columns='Date Time').columns\n",
    "    out_df = df.copy()\n",
    "    for col in columns_to_edit:\n",
    "        out_df[col] = df[col].rolling(window=length).mean()\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca3e6a9-eb16-4b18-8996-4a448b7461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df1, df2):\n",
    "    \"\"\"\n",
    "    Standardize 2 df with same columns\n",
    "    Combines vertically, standardize, then separate and return\n",
    "    Returns the two seprate std df and 2 df with mean and with std.dev in each column\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df_to_standardize = df.drop(columns=['Date Time'], inplace=False)\n",
    "    standardized_values = scaler.fit_transform(df_to_standardize)\n",
    "\n",
    "    # Create standardized DataFrame\n",
    "    df = pd.concat([df['Date Time'], pd.DataFrame(standardized_values, columns=df_to_standardize.columns)], axis=1)\n",
    "\n",
    "    # Compute standard deviation of each column\n",
    "    mean_df = pd.DataFrame(df_to_standardize.mean(), columns=['Mean']).T\n",
    "    std_dev_df = pd.DataFrame(df_to_standardize.std(), columns=['Standard Deviation']).T\n",
    "\n",
    "    df.columns = df.columns\n",
    "    df1_out = df.iloc[:len(df1)].reset_index(drop=True)\n",
    "    df2_out = df.iloc[len(df1):].reset_index(drop=True)\n",
    "    \n",
    "    return df1_out, df2_out, std_dev_df, mean_df\n",
    "    \n",
    "def revert_standardize(df, std_dev_df, mean_df):\n",
    "    \"\"\"\n",
    "    From a standardized df and its means and std.dev, reverts back to normal scale values\n",
    "    \"\"\"\n",
    "    columns = df.drop(columns=['Date Time'], inplace=False).columns\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    # Apply the inverse transformation\n",
    "    df_original[columns] = (df_original[columns] * std_dev_df.loc['Standard Deviation', columns]) + mean_df.loc['Mean', columns]\n",
    "    return df_original\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c2be36-8733-464f-97d7-0bf0d30876eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a total column, used primarily for precipitation\n",
    "def create_total_col(df):\n",
    "    \"\"\"\n",
    "    Adds on a 'total' column\n",
    "    Used for precipitaiton df when calculating dry period\n",
    "    \"\"\"\n",
    "    df['total'] = df.loc[:, df.columns[df.columns.get_loc('Date Time') + 1:]].sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9efe1e-cab0-434e-95d9-8f2c89244020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_dry_period(df, dry_length = 8, days_after = 7, precip_threshold = 0):  \n",
    "    \"\"\"\n",
    "    Finds dry period from precipitation df. \n",
    "    Can select the length of days there must be without rain to be \"dry\"(dry_length)\n",
    "    The grace period where data isnt used. Only days after this period is used (days_after)\n",
    "    and what amount of precipitation is considered a rainfall (precip_threshold)\n",
    "    \"\"\"\n",
    "    zero_ranges = []\n",
    "    in_zero_sequence = False\n",
    "    start_date = None\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['total'] <= precip_threshold:  \n",
    "            if not in_zero_sequence:\n",
    "                # Start of a new zero sequence\n",
    "                in_zero_sequence = True\n",
    "                start_date = row['Date Time']\n",
    "        else:  # Found a non-zero value\n",
    "            if in_zero_sequence:\n",
    "                # End of the zero sequence\n",
    "                end_date = row['Date Time'] - timedelta(days=1) \n",
    "                date_diff = (end_date - start_date).days\n",
    "                if date_diff >= dry_length:  \n",
    "                    zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "                in_zero_sequence = False\n",
    "                start_date = None\n",
    "    \n",
    "    # Handle case where the last sequence of zeros extends to the last row\n",
    "    if in_zero_sequence:\n",
    "        end_date = row['Date Time'] \n",
    "        date_diff = (end_date - start_date).days\n",
    "        if date_diff >= dry_length:\n",
    "            zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "    return zero_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e70c5f-6390-4ccf-8eda-d6a9efe69ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dry_period(df, dry_period):\n",
    "    \"\"\"\n",
    "    Selects rows that are within the given dry_period\n",
    "    \"\"\"\n",
    "    mask = reduce(lambda x, y: x | y, [(df['Date Time'].between(start, end)) for start, end in dry_period])\n",
    "    \n",
    "    filtered_df = df[mask].reset_index(drop=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d06fe8-9f16-4113-97eb-c41fbfb3877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_days_of_week(df):\n",
    "    \"\"\"\n",
    "    Calculates the average of each hour and day of week, and returns values. [Sun 0:00AM to Sat 23:00PM]\n",
    "    Subtracts min() from each column, so min is 0\n",
    "    \"\"\"\n",
    "    # Group by weekday (Sunday=0, Saturday=6) and hour\n",
    "    df['weekday'] = df['Date Time'].dt.weekday  # Monday=0, Sunday=6\n",
    "    df['hour'] = df['Date Time'].dt.hour\n",
    "    \n",
    "    # Adjust weekday order to start from Sunday (moving Sunday=6 to 0)\n",
    "    df['weekday'] = (df['weekday'] + 1) % 7  # Convert Monday=0 â†’ Sunday=0\n",
    "    \n",
    "    hourly_avg = df.groupby(['weekday', 'hour']).mean()\n",
    "    df = df.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg = hourly_avg.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg.columns = df.columns\n",
    "    \n",
    "    # # Sort the values correctly from Sunday 00:00 to Saturday 23:00\n",
    "    hourly_avg = hourly_avg.sort_values(by=['weekday', 'hour'])\n",
    "\n",
    "    # just a refernce datetime from Sun 0:00 to Sat 23:00\n",
    "    date_range = pd.date_range(start=\"2025-02-02 00:00\", end=\"2025-02-08 23:00\", freq=\"h\")\n",
    "    \n",
    "    hourly_avg['Date Time'] = date_range\n",
    "    \n",
    "    num_cols = [col for col in hourly_avg.columns if col != 'Date Time']\n",
    "    hourly_avg[num_cols] = hourly_avg[num_cols] - hourly_avg[num_cols].min()\n",
    "    \n",
    "    return hourly_avg[['Date Time'] + [col for col in hourly_avg.columns if col != 'Date Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a283fe39-f8ea-47ca-a3b0-cf066dfbb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_diurnal(orig_df, correction_df): \n",
    "    \"\"\"\n",
    "    For each datapoint in orig_df, it subtracts the corresponding value from correction_df that has the same hour and day of week from.     \n",
    "    \"\"\"\n",
    "    columns_to_rename = correction_df.drop(columns='Date Time').columns\n",
    "    correction_df.columns = [f'{col} new' if col in columns_to_rename else col for col in correction_df.columns]\n",
    "    \n",
    "    orig_df['weekday'] = orig_df['Date Time'].dt.weekday\n",
    "    correction_df['weekday'] = correction_df['Date Time'].dt.weekday\n",
    "    \n",
    "    orig_df['hour'] = orig_df['Date Time'].dt.hour\n",
    "    correction_df['hour'] = correction_df['Date Time'].dt.hour\n",
    "    \n",
    "    correction_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "\n",
    "    # Set index for main DataFrame to match correction DataFrame\n",
    "    orig_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "    \n",
    "    # Reindex correction DataFrame to match the index of main DataFrame\n",
    "    correction_df = correction_df.reindex(orig_df.index)\n",
    "    \n",
    "    # Concatenate both DataFrames to align corrections with the main DataFrame\n",
    "    removed_df = pd.concat([orig_df, correction_df], axis=1)\n",
    "    for col in orig_df.drop(columns='Date Time').columns:      \n",
    "        removed_df[col] = removed_df[col] - removed_df[f'{col} new']\n",
    "    \n",
    "    removed_df.reset_index(drop=True, inplace=True)\n",
    "    difference_df = removed_df.drop(columns=orig_df.columns)\n",
    "    removed_df = removed_df[orig_df.columns]\n",
    "    return removed_df.iloc[:, [i for i in range(removed_df.shape[1]) if i != 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39515783-d46f-4cfe-84e6-f2fa124c7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_df(time_series_df, df, num):\n",
    "    \"\"\"\n",
    "    repeats the daily avg df so it's easier to compare when aligned with other df\n",
    "    \"\"\"\n",
    "    extended_df = pd.concat([df] * num, ignore_index=True)\n",
    "    extended_df['Date Time'] = time_series_df['Date Time']\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8448787-2a47-4376-9c32-7f8bf43a1d41",
   "metadata": {},
   "source": [
    "# Applying Lyne-Hollick Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89de0305-f1a2-4ab5-86ac-e9117777f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mirrored_rows(df, num_rows=30):\n",
    "    \"\"\"\n",
    "    Insert chronologically mirrored data point at head and tail of df, so Lyne-Hollick doesnt lose affect at ends of data\n",
    "    \"\"\"\n",
    "    mirrored_rows_head = df.iloc[:num_rows].copy()\n",
    "    mirrored_rows_head = mirrored_rows_head.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    mirrored_rows_tail = df.iloc[-num_rows:].copy()\n",
    "    mirrored_rows_tail = mirrored_rows_tail.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    # Concatenate the mirrored rows at the bottom of the original DataFrame\n",
    "    df_extended = pd.concat([mirrored_rows_head, df, mirrored_rows_tail], ignore_index=True)\n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "622097f0-5cde-4ade-aaae-daa42a179916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyne_hollick(df, passes=1, alpha=0.925):\n",
    "    \"\"\"\n",
    "    Applies Lyne-Hollick filter recursively\n",
    "    This method takes a super long time to compile, so if anyone has any idea on improvement, please let me know\n",
    "    \"\"\"\n",
    "    if passes < 1:\n",
    "        return df\n",
    "\n",
    "    out_df = df.copy()\n",
    "\n",
    "    if passes % 2 == 0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    # Handle MultiIndex by flattening if necessary\n",
    "    if isinstance(out_df.columns, pd.MultiIndex):\n",
    "        out_df.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in out_df.columns]\n",
    "    print(\"pass\")\n",
    "    # Select columns excluding 'Date Time'\n",
    "    columns_to_rename = [col for col in out_df.columns if col != 'Date Time']\n",
    "    # This portion takes REALLY long. If anyone has insight on improving it, I would love to discuss\n",
    "    for col in columns_to_rename: \n",
    "        out_df[f'{col} new'] = out_df[col] \n",
    "        for i in range(1, len(out_df)):\n",
    "            out_df.at[i, f'{col} new'] = out_df.at[i, col] - np.maximum(\n",
    "                alpha*(out_df.at[i-1,col] - out_df.at[i-1,f'{col} new']) + ((1+alpha)/2)*(out_df.at[i,col] - out_df.at[i-1,col]),\n",
    "                0)\n",
    "            if pd.isna(out_df.at[i, f'{col} new']):\n",
    "                out_df.at[i, f'{col} new'] = out_df.at[i, col]\n",
    "            elif out_df.at[i, f'{col} new'] <= 0: #need to make value dynamic\n",
    "                out_df.at[i, f'{col} new'] = out_df.at[i-1, f'{col} new']\n",
    "    print(passes)\n",
    "    new_columns = [f'{col} new' for col in columns_to_rename]\n",
    "    out_df = out_df[['Date Time'] + new_columns]\n",
    "\n",
    "    # Match original column names for compatibility\n",
    "    out_df.columns = df[['Date Time'] + columns_to_rename].columns\n",
    "\n",
    "    if passes % 2 == 0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    if passes > 1:\n",
    "        return lyne_hollick(out_df, passes - 1, alpha)\n",
    "    else:\n",
    "        return out_df.iloc[30:-30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e23fd8b-0df4-4a65-a148-08587efa9991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n",
      "1\n",
      "7\n",
      "1\n",
      "6\n",
      "1\n",
      "5\n",
      "1\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# actually using the methods\n",
    "USGS_stream_flow_all_df\n",
    "MMSD_sewerflow_all_df\n",
    "MMSD_flow_and_precip_all_df\n",
    "MMSD_precip_all_df\n",
    "\n",
    "# Create total precipitation level column to find dry season\n",
    "MMSD_precip_all_df = create_total_col(MMSD_precip_all_df)\n",
    "\n",
    "# Find dry period from precipitation data\n",
    "MMSD_precip_dry_periods = find_consecutive_dry_period(MMSD_precip_all_df, 10, 9, 0.0)\n",
    "\n",
    "# Extract dry season data from sewer flow to get dry season diurnal data\n",
    "MMSD_sewerflow_dry_df = select_dry_period(MMSD_sewerflow_all_df, MMSD_precip_dry_periods)\n",
    "\n",
    "# Get avg from [Sunday 0:00 to Saturday 23:00] of the dry season sewer flow\n",
    "MMSD_sewerflow_dry_dailyavg_df = avg_days_of_week(MMSD_sewerflow_dry_df)\n",
    "\n",
    "# Subtract the average from the full period sewer flow\n",
    "MMSD_sewerflow_all_removed_df = subtract_diurnal(MMSD_sewerflow_all_df, MMSD_sewerflow_dry_dailyavg_df)\n",
    "\n",
    "# Apply 7 passes of Lyne-Hollick\n",
    "MMSD_sewerflow_all_removed_7pass_df = lyne_hollick(insert_mirrored_rows(MMSD_sewerflow_all_removed_df), 7, 0.925)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a805e62-011e-47ac-872e-aefa91920667",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMSD_sewerflow_all_removed_7pass_csv = MMSD_sewerflow_all_removed_7pass_df.to_csv('MMSD_sewerflow_all_removed_7pass_df.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f35c6b-ef81-4f5d-8a29-4bc8c93f4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# rewrite code for removing diurnal variation -> use FFT\n",
    "# have to use processed precip data instead of day cumulative -> find hourly cumulation instead of daily\n",
    "\n",
    "# for removing diurnal variation\n",
    "# during dry season when theres no rain, is there no base water flow? -> assume diurnal is 0 at min\n",
    "# lyne hollick\n",
    "# maybe need another function since precipitation lingers quite longer -> apply other filter before lh\n",
    "# the shape of orig function is very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b6130-5d9a-40ce-b703-84733e119b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
