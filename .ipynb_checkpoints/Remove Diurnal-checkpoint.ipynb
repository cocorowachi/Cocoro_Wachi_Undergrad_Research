{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dc144a-93e0-4634-a8e3-20b03da9c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta, SU\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1786cd50-9ee3-409a-b4ce-62b8634a56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = \"C:/Users/wachic/OneDrive - Milwaukee School of Engineering/Documents/GitHub/Undergrad_Research/\"\n",
    "\n",
    "# Naming convention\n",
    "# MMSD_sewerflow_all_dailyavg_df\n",
    "# 1.  [MMSD, USGS] Where the source is\n",
    "# 2.  [sewerflow, precip, streamflow] What the data measures\n",
    "# 3.  [all, dry, wet] what season it includes\n",
    "# 4-n What ever operation has been done to the data\n",
    "# n+1 [df, periods, csv] data type\n",
    "\n",
    "USGS_stream_flow_all_df = pd.read_csv(working_directory + \"USGS 04087030 Streamflow Cleaned.csv\")\n",
    "MMSD_sewerflow_all_df = pd.read_csv(working_directory + \"MMSD Sewer Flow Cleaned.csv\")\n",
    "MMSD_flow_and_precip_all_df = pd.read_csv(working_directory + \"MMSD Flow and Precipitation Cleaned.csv\")\n",
    "MMSD_precip_all_df = pd.read_csv(working_directory + \"MMSD Precipitation Raw Data Cleaned.csv\")\n",
    "\n",
    "df_list = [USGS_stream_flow_all_df,\n",
    "           MMSD_sewerflow_all_df,\n",
    "           MMSD_flow_and_precip_all_df,\n",
    "           MMSD_precip_all_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00526b66-cf47-4885-a1da-fea3b96f2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Date Time'] = pd.to_datetime(df['Date Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb768125-e2fa-46f6-ad1f-c1c3f918ddb6",
   "metadata": {},
   "source": [
    "# Removing Diurnal Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc3d75d-9f1d-463c-a34b-35616f2c8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(df, name='export_df'):\n",
    "    csv = df.to_csv(f'{name}.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5dda9d1-f0d2-493e-af79-8987e00d91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_one(df):\n",
    "    df = df.copy()\n",
    "    df.iloc[:, 1:] = df.iloc[:, 1:].shift(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c2be36-8733-464f-97d7-0bf0d30876eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_total_col(df):\n",
    "    \"\"\"\n",
    "    Adds on a 'total' column\n",
    "    Used for precipitaiton df when calculating dry period\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['total'] = df.loc[:, df.columns[df.columns.get_loc('Date Time') + 1:]].sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79dfd537-8346-4ff1-bc31-d988ae01bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(df1, df2):\n",
    "    \"\"\"\n",
    "    create df that is the difference of two df\n",
    "    \"\"\"\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    columns_to_subtract =  [col for col in df1.columns if col != 'Date Time']\n",
    "\n",
    "    df_diff = df1.copy()  \n",
    "    df_diff[columns_to_subtract] = df1[columns_to_subtract] - df2[columns_to_subtract]\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6840a22a-a5e2-4486-9844-a7d6f29a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_na(edit_df, with_na_df):\n",
    "    \"\"\"\n",
    "    re-sets any nan values from\n",
    "    \"\"\"\n",
    "    edit_df = edit_df.copy()\n",
    "    with_na_df = with_na_df.copy()\n",
    "    edit_df[with_na_df.isna()] = np.nan\n",
    "    return edit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39515783-d46f-4cfe-84e6-f2fa124c7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_df(time_series_df, df, num):\n",
    "    \"\"\"\n",
    "    repeats the daily avg df so it's easier to compare when aligned with other df\n",
    "    \"\"\"\n",
    "    time_series_df = time_series_df.copy()\n",
    "    df = df.copy()\n",
    "    extended_df = pd.concat([df] * num, ignore_index=True)\n",
    "    extended_df['Date Time'] = time_series_df['Date Time']\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca3e6a9-eb16-4b18-8996-4a448b7461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df1, df2):\n",
    "    \"\"\"\n",
    "    Standardize 2 df with same columns\n",
    "    Combines vertically, standardize, then separate and return\n",
    "    Returns the two seprate std df and 2 df with mean and with std.dev in each column\n",
    "    \"\"\"\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    scaler = StandardScaler()\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df_to_standardize = df.drop(columns=['Date Time'], inplace=False)\n",
    "    standardized_values = scaler.fit_transform(df_to_standardize)\n",
    "\n",
    "    # Create standardized DataFrame\n",
    "    df = pd.concat([df['Date Time'], pd.DataFrame(standardized_values, columns=df_to_standardize.columns)], axis=1)\n",
    "\n",
    "    # Compute mean and standard deviation of each column\n",
    "    mean_df = pd.DataFrame(df_to_standardize.mean(), columns=['Mean']).T\n",
    "    std_dev_df = pd.DataFrame(df_to_standardize.std(), columns=['Standard Deviation']).T\n",
    "\n",
    "    df.columns = df.columns\n",
    "    df1_out = df.iloc[:len(df1)].reset_index(drop=True)\n",
    "    df2_out = df.iloc[len(df1):].reset_index(drop=True)\n",
    "    \n",
    "    return df1_out, df2_out, std_dev_df, mean_df\n",
    "    \n",
    "def revert_standardize(df, std_dev_df, mean_df):\n",
    "    \"\"\"\n",
    "    From a standardized df and its means and std.dev, reverts back to normal scale values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    std_dev_df = std_dev_df.copy()\n",
    "    mean_df = mean_df.copy()\n",
    "    columns = [col for col in df1.columns if col != 'Date Time']\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    df_original[columns] = (df_original[columns] * std_dev_df.loc['Standard Deviation', columns]) + mean_df.loc['Mean', columns]\n",
    "    return df_original\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9efe1e-cab0-434e-95d9-8f2c89244020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_consecutive_dry_period(df, dry_length = 8, days_after = 7, precip_threshold = 0):  \n",
    "#     df = df.copy()\n",
    "#     \"\"\"\n",
    "#     Finds dry period from precipitation df. \n",
    "#     Can select the length of days there must be without rain to be \"dry\"(dry_length)\n",
    "#     The grace period where data isnt used. Only days after this period is used (days_after)\n",
    "#     and what amount of precipitation is considered a rainfall (precip_threshold)\n",
    "#     \"\"\"\n",
    "#     zero_ranges = []\n",
    "#     in_zero_sequence = False\n",
    "#     start_date = None\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         if row['total'] <= precip_threshold:  \n",
    "#             if not in_zero_sequence:\n",
    "#                 # Start of a new zero sequence\n",
    "#                 in_zero_sequence = True\n",
    "#                 start_date = row['Date Time']\n",
    "#         else:  # Found a non-zero value\n",
    "#             if in_zero_sequence:\n",
    "#                 # End of the zero sequence\n",
    "#                 end_date = row['Date Time'] - timedelta(days=1) \n",
    "#                 date_diff = (end_date - start_date).days\n",
    "#                 if date_diff >= dry_length:  \n",
    "#                     zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "#                 in_zero_sequence = False\n",
    "#                 start_date = None\n",
    "    \n",
    "#     # Handle case where the last sequence of zeros extends to the last row\n",
    "#     if in_zero_sequence:\n",
    "#         end_date = row['Date Time'] \n",
    "#         date_diff = (end_date - start_date).days\n",
    "#         if date_diff >= dry_length:\n",
    "#             zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "#     return zero_ranges\n",
    "\n",
    "\n",
    "def find_consecutive_dry_period(df, dry_length=8, days_after=7, precip_threshold=0):\n",
    "    \"\"\"\n",
    "    Finds dry period from precipitation df. \n",
    "    Can select the length of days there must be without rain to be \"dry\"(dry_length)\n",
    "    The grace period where data isnt used. Only days after this period is used (days_after)\n",
    "    and what amount of precipitation is considered a rainfall (precip_threshold)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create a boolean series where True represents dry days\n",
    "    dry_days = df['total'] <= precip_threshold\n",
    "\n",
    "    # Find the start and end indices of consecutive dry periods\n",
    "    df['dry_group'] = (dry_days != dry_days.shift()).cumsum() * dry_days\n",
    "\n",
    "    dry_periods = df[df['dry_group'] > 0].groupby('dry_group').agg(\n",
    "        start_date=('Date Time', 'first'),\n",
    "        end_date=('Date Time', 'last'),\n",
    "        length=('Date Time', 'size')\n",
    "    )\n",
    "\n",
    "    # Filter by dry length\n",
    "    dry_periods = dry_periods[dry_periods['length'] >= dry_length]\n",
    "\n",
    "    # Apply days_after adjustment\n",
    "    dry_periods['start_date'] += pd.to_timedelta(days_after, unit='D')\n",
    "\n",
    "    # Return as list of tuples\n",
    "    zero_ranges = list(zip(dry_periods['start_date'], dry_periods['end_date']))\n",
    "\n",
    "    return zero_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e70c5f-6390-4ccf-8eda-d6a9efe69ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dry_period(df, dry_period):\n",
    "    \"\"\"\n",
    "    Selects rows that are within the given dry_period\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    mask = reduce(lambda x, y: x | y, [(df['Date Time'].between(start, end)) for start, end in dry_period])\n",
    "    \n",
    "    filtered_df = df[mask].reset_index(drop=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d06fe8-9f16-4113-97eb-c41fbfb3877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_days_of_week(df):\n",
    "    \"\"\"\n",
    "    Calculates the average of each hour and day of week, and returns values. [Sun 0:00AM to Sat 23:00PM]\n",
    "    Subtracts min() from each column, so min is 0\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Group by weekday (Sunday=0, Saturday=6) and hour\n",
    "    df['weekday'] = df['Date Time'].dt.weekday  # Monday=0, Sunday=6\n",
    "    df['hour'] = df['Date Time'].dt.hour\n",
    "    \n",
    "    # Adjust weekday order to start from Sunday (moving Sunday=6 to 0)\n",
    "    df['weekday'] = (df['weekday'] + 1) % 7  # Convert Monday=0 â†’ Sunday=0\n",
    "    \n",
    "    hourly_avg = df.groupby(['weekday', 'hour']).mean()\n",
    "    df = df.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg = hourly_avg.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg.columns = df.columns\n",
    "    \n",
    "    # # Sort the values correctly from Sunday 00:00 to Saturday 23:00\n",
    "    hourly_avg = hourly_avg.sort_values(by=['weekday', 'hour'])\n",
    "\n",
    "    # Just a refernce datetime from Sun 0:00 to Sat 23:00\n",
    "    date_range = pd.date_range(start=\"2025-02-02 00:00\", end=\"2025-02-08 23:00\", freq=\"h\")\n",
    "    \n",
    "    hourly_avg['Date Time'] = date_range\n",
    "    \n",
    "    num_cols = [col for col in hourly_avg.columns if col != 'Date Time']\n",
    "    hourly_avg[num_cols] = hourly_avg[num_cols] - hourly_avg[num_cols].min()\n",
    "    \n",
    "    return hourly_avg[['Date Time'] + [col for col in hourly_avg.columns if col != 'Date Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a283fe39-f8ea-47ca-a3b0-cf066dfbb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_diurnal(orig_df, correction_df):\n",
    "    \"\"\"\n",
    "    Subtracts the corresponding hourly and weekday values from correction_df from orig_df for each data point.\n",
    "    \"\"\"\n",
    "    orig_df = orig_df.copy()\n",
    "    correction_df = correction_df.copy()\n",
    "\n",
    "    columns_to_subtract = [col for col in orig_df.columns if col != 'Date Time']\n",
    "\n",
    "    orig_df['weekday'] = orig_df['Date Time'].dt.weekday\n",
    "    orig_df['hour'] = orig_df['Date Time'].dt.hour\n",
    "    correction_df['weekday'] = correction_df['Date Time'].dt.weekday\n",
    "    correction_df['hour'] = correction_df['Date Time'].dt.hour\n",
    "\n",
    "    # Set multi-index on weekday and hour\n",
    "    orig_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "    correction_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "\n",
    "    # Reindex correction_df to align with orig_df\n",
    "    correction_df = correction_df.reindex(orig_df.index)\n",
    "\n",
    "    orig_numeric = orig_df[columns_to_subtract]\n",
    "    correction_numeric = correction_df[columns_to_subtract]\n",
    "\n",
    "    # Perform subtraction in a vectorized manner\n",
    "    result_numeric = orig_numeric - correction_numeric\n",
    "\n",
    "    result_numeric.reset_index(inplace=True)\n",
    "    orig_df.reset_index(inplace=True)\n",
    "\n",
    "    # Add 'Date Time' column back to result\n",
    "    result_numeric.insert(0, 'Date Time', orig_df['Date Time'])\n",
    "\n",
    "    # Drop 'weekday' and 'hour' columns\n",
    "    result_numeric.drop(columns=['weekday', 'hour'], inplace=True)\n",
    "    return result_numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4a2a9e-2f89-4f1e-a390-40e099519db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_diurnal(df, ref_df):\n",
    "    \"\"\"\n",
    "    From a df of estimated diurnal curves, \n",
    "    selects 'normal' diurnal curve which fits between 0 and max value from previous diurnal\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    ref_df = ref_df.copy()\n",
    "    df.set_index('Date Time', inplace=True)\n",
    "    ref_df.set_index('Date Time', inplace=True)\n",
    "    ref_daily_max = ref_df['MS0311 Flow'].resample('D').max()*1.1\n",
    "\n",
    "    # Function to compare daily max threshold\n",
    "    def day_below_max(x):\n",
    "        day = x.index[0].normalize()\n",
    "        max_threshold = ref_daily_max.get(day, float('inf'))  # Default to inf if day is missing\n",
    "        return x['MS0311 Flow'].max() <= max_threshold\n",
    "\n",
    "    # Filter days where max is <= daily max from ref_df\n",
    "    days_with_min_0 = df.groupby(df.index.normalize()).filter(day_below_max)\n",
    "\n",
    "    # Reset index to bring back 'Date Time' as a column\n",
    "    days_with_min_0 = days_with_min_0.reset_index()\n",
    "\n",
    "    return days_with_min_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89de0305-f1a2-4ab5-86ac-e9117777f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mirrored_rows(df, num_rows=30):\n",
    "    \"\"\"\n",
    "    Insert chronologically mirrored data point at head and tail of df\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    mirrored_rows_head = df.iloc[:num_rows].copy()\n",
    "    mirrored_rows_head = mirrored_rows_head.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    mirrored_rows_tail = df.iloc[-num_rows:].copy()\n",
    "    mirrored_rows_tail = mirrored_rows_tail.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    df_extended = pd.concat([mirrored_rows_head, df, mirrored_rows_tail], ignore_index=True)\n",
    "    \n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5fb33-9eb2-4e8f-b74a-6cba640f6956",
   "metadata": {},
   "source": [
    "### Smoothing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b36fa2-b8af-4c7d-b5fc-3f2de4a06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(df, length=24):\n",
    "    \"\"\" \n",
    "    Finds moving average for past 24 hour\n",
    "    \"\"\"\n",
    "    df = insert_mirrored_rows(df.copy())\n",
    "    # columns_to_edit = [col for col in df.columns if col != 'Date Time']\n",
    "    # out_df = df.copy()\n",
    "    # for col in columns_to_edit:\n",
    "    #     out_df[col] = df[col].rolling(window=length).mean()\n",
    "    out_df = df.set_index('Date Time').rolling(window=length).mean().reset_index()\n",
    "    return out_df.iloc[30:-30].reset_index(drop=True)\n",
    "    \n",
    "def moving_avg_mid(df, lengths=[3]):\n",
    "    \"\"\" \n",
    "    Finds centered moving averages for given window lengths.\n",
    "    Overwrites the same columns on each iteration.\n",
    "    \"\"\"\n",
    "    df = insert_mirrored_rows(df.copy())\n",
    "    columns_to_edit = [col for col in df.columns if col != 'Date Time']\n",
    "    numeric_df = df[columns_to_edit]\n",
    "    for length in lengths:\n",
    "        df[columns_to_edit] = df[columns_to_edit].rolling(window=length, center=True).mean()\n",
    "\n",
    "    return df.iloc[30:-30].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849aac59-8d85-4d92-b017-d7f5f737a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCHIP(df, passes=1):\n",
    "    \"\"\"\n",
    "    A filter method that connects all local minima with a smoothing function\n",
    "    Number of passes doesn't have to be odd, can be even\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.ffill(inplace=True)\n",
    "    timestamps = df['Date Time'].astype(np.int64) // 10**9  # to seconds\n",
    "    timestamps_linspace = np.linspace(timestamps.min(), timestamps.max(), len(df))\n",
    "    x = timestamps_linspace\n",
    "\n",
    "    def apply_pchip(col):\n",
    "        y = col.to_numpy()\n",
    "        \n",
    "        # Identify local minima\n",
    "        local_minima_indices = argrelextrema(y, np.less)[0]\n",
    "        \n",
    "        # Extract x and y values at local minima\n",
    "        x_minima = x[local_minima_indices]\n",
    "        y_minima = y[local_minima_indices]\n",
    "        \n",
    "        # Create a PCHIP interpolator\n",
    "        pchp_interpolator = PchipInterpolator(x_minima, y_minima)\n",
    "        \n",
    "        # Evaluate the interpolator on the original x values\n",
    "        return pchp_interpolator(x)\n",
    "    columns_to_work = [col for col in df.columns if col != 'Date Time']\n",
    "    # Apply PCHIP interpolation to all columns (except 'Date Time')\n",
    "    df[columns_to_work] = df[columns_to_work].apply(apply_pchip)\n",
    "\n",
    "    if passes > 1:\n",
    "        return PCHIP(df, passes-1)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def PCHIP_init(df, passes=1):\n",
    "    \"\"\"\n",
    "    Call this method for practicality, won't lose accuracy in beginning and end by adding padding\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    return PCHIP(insert_mirrored_rows(df), passes).iloc[30:-30].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61b2abc9-3274-4537-b50e-cf0aec66b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def lyne_hollick(df, passes=1, alpha=0.925):\n",
    "    \"\"\"\n",
    "    Applies Lyne-Hollick filter recursively with optimization\n",
    "    Number of passes shall be odd\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if passes < 1:\n",
    "        return df\n",
    "\n",
    "    reverse_pass = (passes % 2 == 0)\n",
    "    if reverse_pass:\n",
    "        df = df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    # Flatten MultiIndex if necessary\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [' '.join(map(str, col)).strip() for col in df.columns]\n",
    "\n",
    "    data = df.to_numpy(copy=True)\n",
    "    out = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    for col in range(1, n_cols):\n",
    "        out[0, col] = data[0,col]\n",
    "        prev_out = data[0, col]  # Initialize with the first value\n",
    "        prev_data = data[0, col]\n",
    "        for row in range(1,n_rows):\n",
    "            cur_data = data[row, col]\n",
    "            \n",
    "            term1 = alpha * (prev_data - prev_out)\n",
    "            term2 = ((1 + alpha) / 2) * (cur_data - prev_data)\n",
    "            cur_out = cur_data - max(term1 + term2, 0)\n",
    "            \n",
    "            if np.isnan(cur_out):\n",
    "                cur_out = cur_data\n",
    "            if cur_out <= 0:\n",
    "                cur_out = prev_out\n",
    "            out[row, col] = cur_out\n",
    "            prev_out = cur_out\n",
    "            prev_data = cur_data\n",
    "    print(passes)\n",
    "    out_df = pd.DataFrame(out, columns=df.columns)\n",
    "\n",
    "    if reverse_pass:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    if passes > 1:\n",
    "        return lyne_hollick(out_df, passes - 1, alpha)\n",
    "\n",
    "    return out_df\n",
    "\n",
    "def lyne_hollick_init(df, passes=1, alpha=0.925):\n",
    "    \"\"\"\n",
    "    Call this method for practicality, won't lose accuracy in beginning and end by adding padding\n",
    "    \"\"\"\n",
    "    df_padded = insert_mirrored_rows(df)\n",
    "    result = lyne_hollick(df_padded, passes, alpha)\n",
    "    return result.iloc[30:-30].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fab675d-fd1e-433a-92ae-7b99c5bb4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diurnal(sewer_df, precip_df):\n",
    "    \"\"\"\n",
    "    Just runs all the methods for you\n",
    "    sewer_df: df in which you want to remove diurnal variation\n",
    "    precip_df: precipitation df to determine dry period\n",
    "    \"\"\"\n",
    "    # General flow:\n",
    "    # Step 1. Finds a temporary daily diurnal variation for dry season(determined from precip data),\n",
    "    #         and calculate the hourly average from sunday to saturday\n",
    "    # 2. Calculates temporary smooth function of base flow of sewer data. This is not the output data\n",
    "    # 3. Calculates a rough diurnal variation accross all data by subtracting (raw - smooth)\n",
    "    # 4. Finds the hourly avg for the week from the diurnal curve from 3 (only select days are used to calculate the avg variation\n",
    "    # 5. Subtracts the newly diurnal variation from the raw data\n",
    "    # 6. Smooths out resulting df a little bit\n",
    "    # 7. Re-sets any nan value that was present in the raw data, since they were filled in at somepoint\n",
    "    \n",
    "    # Step 1\n",
    "    precip_all_df = create_total_col(precip_df)\n",
    "    precip_dry_periods = find_consecutive_dry_period(precip_all_df, 10, 9, 0.0)\n",
    "    sewerflow_dry_df = select_dry_period(sewer_df, precip_dry_periods)\n",
    "    sewerflow_dry_dailyavg_df = avg_days_of_week(sewerflow_dry_df)\n",
    "    # 2\n",
    "    sewerflow_movingavgmid_df = moving_avg_mid(sewer_df, [5,5,5])\n",
    "    sewerflow_all_PCHIP2_df = PCHIP_init(sewerflow_movingavgmid_df, 1)\n",
    "    # 3\n",
    "    sewerflow_all_PCHIP2_diurnal_isolated_df = diff(sewer_df, sewerflow_all_PCHIP2_df)\n",
    "    # 4\n",
    "    sewerflow_all_reference_diurnal_df = get_ref_diurnal(sewerflow_all_PCHIP2_diurnal_isolated_df, sewerflow_dry_dailyavg_df)\n",
    "    sewerflow_all_newdiurnal_dailyavg_df = avg_days_of_week(sewerflow_all_reference_diurnal_df)\n",
    "    # 5\n",
    "    sewerflow_all_removed_df = subtract_diurnal(sewer_df, sewerflow_all_newdiurnal_dailyavg_df)\n",
    "    # 6\n",
    "    sewerflow_all_removed_movingCenterAvg_df = moving_avg_mid(sewerflow_all_removed_df, [3,3,3])\n",
    "    # 7\n",
    "    sewerflow_all_removed_withna_df = set_na(sewerflow_all_removed_movingCenterAvg_df, sewer_df)\n",
    "    \n",
    "    return sewerflow_all_removed_withna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d758b839-a772-4641-ba7f-00f22c4bff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 688 ms\n",
      "Wall time: 786 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MMSD_sewerflow_removed_df = remove_diurnal(MMSD_sewerflow_all_df, MMSD_precip_all_df)\n",
    "MMSD_sewerflow_baseflow_df = sewerflow_all_PCHIP2_df = PCHIP_init(moving_avg_mid(MMSD_sewerflow_removed_df, [5,5,5]), 1)\n",
    "MMSD_sewerflow_precipitation_df = diff(MMSD_sewerflow_removed_df, MMSD_sewerflow_baseflow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b282ff8-5eef-485c-858f-7e01f1666ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export(MMSD_sewerflow_removed_df,\"MMSD_sewerflow_removed_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51f35c6b-ef81-4f5d-8a29-4bc8c93f4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# have to use processed precip data instead of day cumulative -> calculate hourly cumulation instead of daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18d57ef8-0588-4021-813d-9993e5c4b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First removed was simply subtracting dry season daily avg\n",
    "# old removed was using lh7\n",
    "# new removed uses pchip2 and smoothing out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
