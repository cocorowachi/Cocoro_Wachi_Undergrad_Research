{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dc144a-93e0-4634-a8e3-20b03da9c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta, SU\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1786cd50-9ee3-409a-b4ce-62b8634a56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = \"C:/Users/wachic/OneDrive - Milwaukee School of Engineering/Desktop/Undergrad Research/\"\n",
    "\n",
    "# Naming convention\n",
    "# MMSD_sewerflow_all_dailyavg_df\n",
    "# 1.  [MMSD, USGS] Where the source is\n",
    "# 2.  [sewerflow, precip, streamflow] What the data measures\n",
    "# 3.  [all, dry, wet] what season it includes\n",
    "# 4-n What ever operation has been done to the data\n",
    "# n+1 [df, periods, csv] data type\n",
    "\n",
    "USGS_stream_flow_all_df = pd.read_csv(working_directory + \"USGS 04087030 Streamflow Cleaned.csv\")\n",
    "MMSD_sewerflow_all_df = pd.read_csv(working_directory + \"MMSD Sewer Flow Cleaned.csv\")\n",
    "MMSD_flow_and_precip_all_df = pd.read_csv(working_directory + \"MMSD Flow and Precipitation Cleaned.csv\")\n",
    "MMSD_precip_all_df = pd.read_csv(working_directory + \"MMSD Precipitation Raw Data Cleaned.csv\")\n",
    "\n",
    "df_list = [USGS_stream_flow_all_df,\n",
    "           MMSD_sewerflow_all_df,\n",
    "           MMSD_flow_and_precip_all_df,\n",
    "           MMSD_precip_all_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00526b66-cf47-4885-a1da-fea3b96f2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Date Time'] = df['Date Time'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb768125-e2fa-46f6-ad1f-c1c3f918ddb6",
   "metadata": {},
   "source": [
    "# Removing Diurnal Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b36fa2-b8af-4c7d-b5fc-3f2de4a06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(df, length=24):\n",
    "    \"\"\" \n",
    "    Finds moving average for past 24 hour\n",
    "    \"\"\"\n",
    "    columns_to_edit = df.drop(columns='Date Time').columns\n",
    "    out_df = df.copy()\n",
    "    for col in columns_to_edit:\n",
    "        out_df[col] = df[col].rolling(window=length).mean()\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca3e6a9-eb16-4b18-8996-4a448b7461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df1, df2):\n",
    "    \"\"\"\n",
    "    Standardize 2 df with same columns\n",
    "    Combines vertically, standardize, then separate and return\n",
    "    Returns the two seprate std df and 2 df with mean and with std.dev in each column\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df_to_standardize = df.drop(columns=['Date Time'], inplace=False)\n",
    "    standardized_values = scaler.fit_transform(df_to_standardize)\n",
    "\n",
    "    # Create standardized DataFrame\n",
    "    df = pd.concat([df['Date Time'], pd.DataFrame(standardized_values, columns=df_to_standardize.columns)], axis=1)\n",
    "\n",
    "    # Compute standard deviation of each column\n",
    "    mean_df = pd.DataFrame(df_to_standardize.mean(), columns=['Mean']).T\n",
    "    std_dev_df = pd.DataFrame(df_to_standardize.std(), columns=['Standard Deviation']).T\n",
    "\n",
    "    df.columns = df.columns\n",
    "    df1_out = df.iloc[:len(df1)].reset_index(drop=True)\n",
    "    df2_out = df.iloc[len(df1):].reset_index(drop=True)\n",
    "    \n",
    "    return df1_out, df2_out, std_dev_df, mean_df\n",
    "    \n",
    "def revert_standardize(df, std_dev_df, mean_df):\n",
    "    \"\"\"\n",
    "    From a standardized df and its means and std.dev, reverts back to normal scale values\n",
    "    \"\"\"\n",
    "    columns = df.drop(columns=['Date Time'], inplace=False).columns\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    # Apply the inverse transformation\n",
    "    df_original[columns] = (df_original[columns] * std_dev_df.loc['Standard Deviation', columns]) + mean_df.loc['Mean', columns]\n",
    "    return df_original\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c2be36-8733-464f-97d7-0bf0d30876eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a total column, used primarily for precipitation\n",
    "def create_total_col(df):\n",
    "    \"\"\"\n",
    "    Adds on a 'total' column\n",
    "    Used for precipitaiton df when calculating dry period\n",
    "    \"\"\"\n",
    "    df['total'] = df.loc[:, df.columns[df.columns.get_loc('Date Time') + 1:]].sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9efe1e-cab0-434e-95d9-8f2c89244020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_dry_period(df, dry_length = 8, days_after = 7, precip_threshold = 0):  \n",
    "    \"\"\"\n",
    "    Finds dry period from precipitation df. \n",
    "    Can select the length of days there must be without rain to be \"dry\"(dry_length)\n",
    "    The grace period where data isnt used. Only days after this period is used (days_after)\n",
    "    and what amount of precipitation is considered a rainfall (precip_threshold)\n",
    "    \"\"\"\n",
    "    zero_ranges = []\n",
    "    in_zero_sequence = False\n",
    "    start_date = None\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['total'] <= precip_threshold:  \n",
    "            if not in_zero_sequence:\n",
    "                # Start of a new zero sequence\n",
    "                in_zero_sequence = True\n",
    "                start_date = row['Date Time']\n",
    "        else:  # Found a non-zero value\n",
    "            if in_zero_sequence:\n",
    "                # End of the zero sequence\n",
    "                end_date = row['Date Time'] - timedelta(days=1) \n",
    "                date_diff = (end_date - start_date).days\n",
    "                if date_diff >= dry_length:  \n",
    "                    zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "                in_zero_sequence = False\n",
    "                start_date = None\n",
    "    \n",
    "    # Handle case where the last sequence of zeros extends to the last row\n",
    "    if in_zero_sequence:\n",
    "        end_date = row['Date Time'] \n",
    "        date_diff = (end_date - start_date).days\n",
    "        if date_diff >= dry_length:\n",
    "            zero_ranges.append((start_date + timedelta(days=days_after), end_date))\n",
    "    return zero_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e70c5f-6390-4ccf-8eda-d6a9efe69ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dry_period(df, dry_period):\n",
    "    \"\"\"\n",
    "    Selects rows that are within the given dry_period\n",
    "    \"\"\"\n",
    "    mask = reduce(lambda x, y: x | y, [(df['Date Time'].between(start, end)) for start, end in dry_period])\n",
    "    \n",
    "    filtered_df = df[mask].reset_index(drop=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d06fe8-9f16-4113-97eb-c41fbfb3877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_days_of_week(df):\n",
    "    \"\"\"\n",
    "    Calculates the average of each hour and day of week, and returns values. [Sun 0:00AM to Sat 23:00PM]\n",
    "    Subtracts min() from each column, so min is 0\n",
    "    \"\"\"\n",
    "    print(df['MS0311 Flow'].min())\n",
    "    # Group by weekday (Sunday=0, Saturday=6) and hour\n",
    "    df['weekday'] = df['Date Time'].dt.weekday  # Monday=0, Sunday=6\n",
    "    df['hour'] = df['Date Time'].dt.hour\n",
    "    \n",
    "    # Adjust weekday order to start from Sunday (moving Sunday=6 to 0)\n",
    "    df['weekday'] = (df['weekday'] + 1) % 7  # Convert Monday=0 â†’ Sunday=0\n",
    "    \n",
    "    hourly_avg = df.groupby(['weekday', 'hour']).mean()\n",
    "    df = df.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg = hourly_avg.drop(columns=['weekday', 'hour'], errors='ignore')\n",
    "    hourly_avg.columns = df.columns\n",
    "    \n",
    "    # # Sort the values correctly from Sunday 00:00 to Saturday 23:00\n",
    "    hourly_avg = hourly_avg.sort_values(by=['weekday', 'hour'])\n",
    "\n",
    "    # just a refernce datetime from Sun 0:00 to Sat 23:00\n",
    "    date_range = pd.date_range(start=\"2025-02-02 00:00\", end=\"2025-02-08 23:00\", freq=\"h\")\n",
    "    \n",
    "    hourly_avg['Date Time'] = date_range\n",
    "    \n",
    "    num_cols = [col for col in hourly_avg.columns if col != 'Date Time']\n",
    "    hourly_avg[num_cols] = hourly_avg[num_cols] - hourly_avg[num_cols].min()\n",
    "    \n",
    "    return hourly_avg[['Date Time'] + [col for col in hourly_avg.columns if col != 'Date Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a283fe39-f8ea-47ca-a3b0-cf066dfbb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_diurnal(orig_df, correction_df): \n",
    "    \"\"\"\n",
    "    For each datapoint in orig_df, it subtracts the corresponding value from correction_df that has the same hour and day of week from.     \n",
    "    \"\"\"\n",
    "    columns_to_rename = correction_df.drop(columns='Date Time').columns\n",
    "    correction_df.columns = [f'{col} new' if col in columns_to_rename else col for col in correction_df.columns]\n",
    "    \n",
    "    orig_df['weekday'] = orig_df['Date Time'].dt.weekday\n",
    "    correction_df['weekday'] = correction_df['Date Time'].dt.weekday\n",
    "    \n",
    "    orig_df['hour'] = orig_df['Date Time'].dt.hour\n",
    "    correction_df['hour'] = correction_df['Date Time'].dt.hour\n",
    "    \n",
    "    correction_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "\n",
    "    # Set index for main DataFrame to match correction DataFrame\n",
    "    orig_df.set_index(['weekday', 'hour'], inplace=True)\n",
    "    \n",
    "    # Reindex correction DataFrame to match the index of main DataFrame\n",
    "    correction_df = correction_df.reindex(orig_df.index)\n",
    "    \n",
    "    # Concatenate both DataFrames to align corrections with the main DataFrame\n",
    "    removed_df = pd.concat([orig_df, correction_df], axis=1)\n",
    "    for col in orig_df.drop(columns='Date Time').columns:      \n",
    "        removed_df[col] = removed_df[col] - removed_df[f'{col} new']\n",
    "    \n",
    "    removed_df.reset_index(drop=True, inplace=True)\n",
    "    difference_df = removed_df.drop(columns=orig_df.columns)\n",
    "    removed_df = removed_df[orig_df.columns]\n",
    "    return removed_df.iloc[:, [i for i in range(removed_df.shape[1]) if i != 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39515783-d46f-4cfe-84e6-f2fa124c7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_df(time_series_df, df, num):\n",
    "    \"\"\"\n",
    "    repeats the daily avg df so it's easier to compare when aligned with other df\n",
    "    \"\"\"\n",
    "    extended_df = pd.concat([df] * num, ignore_index=True)\n",
    "    extended_df['Date Time'] = time_series_df['Date Time']\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8448787-2a47-4376-9c32-7f8bf43a1d41",
   "metadata": {},
   "source": [
    "# Applying Lyne-Hollick Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89de0305-f1a2-4ab5-86ac-e9117777f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mirrored_rows(df, num_rows=30):\n",
    "    \"\"\"\n",
    "    Insert chronologically mirrored data point at head and tail of df, so Lyne-Hollick doesnt lose affect at ends of data\n",
    "    \"\"\"\n",
    "    mirrored_rows_head = df.iloc[:num_rows].copy()\n",
    "    mirrored_rows_head = mirrored_rows_head.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    mirrored_rows_tail = df.iloc[-num_rows:].copy()\n",
    "    mirrored_rows_tail = mirrored_rows_tail.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    # Concatenate the mirrored rows at the bottom of the original DataFrame\n",
    "    df_extended = pd.concat([mirrored_rows_head, df, mirrored_rows_tail], ignore_index=True)\n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "622097f0-5cde-4ade-aaae-daa42a179916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyne_hollick(df, passes=1, alpha=0.925):\n",
    "    \n",
    "    if passes < 1:\n",
    "        return df\n",
    "    out_df = df.copy()\n",
    "    if passes%2==0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    columns_to_rename = df.copy().drop(columns='Date Time').columns\n",
    "    # out_df.columns = [col + ' new' if col in columns_to_rename else col for col in out_df.columns]\n",
    "    for col in columns_to_rename: \n",
    "        out_df[f'{col}_y'] = out_df[col] \n",
    "        for i in range(1, len(out_df)):\n",
    "            out_df.at[i, f'{col}_y'] = out_df.at[i, col] - np.maximum(\n",
    "                alpha*(out_df.at[i-1,col] - out_df.at[i-1,f'{col}_y']) + ((1+alpha)/2)*(out_df.at[i,col] - out_df.at[i-1,col]),\n",
    "                0)\n",
    "            if pd.isna(out_df.at[i, f'{col}_y']):\n",
    "                out_df.at[i, f'{col}_y'] = out_df.at[i, col]\n",
    "            elif out_df.at[i, f'{col}_y'] <= 0: #need to make value dynamic\n",
    "                out_df.at[i, f'{col}_y'] = out_df.at[i-1, f'{col}_y']\n",
    "    new_columns = [f'{col}_y' for col in columns_to_rename]\n",
    "    # out_df = out_df['Date Time', list(new_columns)]\n",
    "    out_df = out_df[['Date Time'] + new_columns]\n",
    "    out_df.columns = [df.columns]\n",
    "    if passes%2==0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if passes > 1:\n",
    "        return lyne_hollick(out_df, passes-1, alpha)\n",
    "    else:\n",
    "        return out_df.iloc[30:-30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e23fd8b-0df4-4a65-a148-08587efa9991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wachic\\AppData\\Local\\Temp\\ipykernel_27360\\2443817340.py:11: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  columns_to_rename = df.copy().drop(columns='Date Time').columns\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m MMSD_sewerflow_all_removed_df \u001b[38;5;241m=\u001b[39m subtract_diurnal(MMSD_sewerflow_all_df, MMSD_sewerflow_dry_dailyavg_df)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply 7 passes of Lyne-Hollick\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m MMSD_sewerflow_all_removed_7pass_df \u001b[38;5;241m=\u001b[39m \u001b[43mlyne_hollick\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_mirrored_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMMSD_sewerflow_all_removed_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.925\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mlyne_hollick\u001b[1;34m(df, passes, alpha)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m passes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlyne_hollick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m30\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m]\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mlyne_hollick\u001b[1;34m(df, passes, alpha)\u001b[0m\n\u001b[0;32m     14\u001b[0m out_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_y\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out_df[col] \n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(out_df)):\n\u001b[0;32m     16\u001b[0m     out_df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_y\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out_df\u001b[38;5;241m.\u001b[39mat[i, col] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\n\u001b[1;32m---> 17\u001b[0m         alpha\u001b[38;5;241m*\u001b[39m(out_df\u001b[38;5;241m.\u001b[39mat[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,col] \u001b[38;5;241m-\u001b[39m \u001b[43mout_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39malpha)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39m(out_df\u001b[38;5;241m.\u001b[39mat[i,col] \u001b[38;5;241m-\u001b[39m out_df\u001b[38;5;241m.\u001b[39mat[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,col]),\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(out_df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_y\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     20\u001b[0m         out_df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_y\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out_df\u001b[38;5;241m.\u001b[39mat[i, col]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\pandas\\core\\indexing.py:2575\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key]\n\u001b[1;32m-> 2575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\pandas\\core\\indexing.py:2527\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2526\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\pandas\\core\\frame.py:4214\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4211\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[1;32m-> 4214\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\pandas\\core\\frame.py:4639\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4635\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[0;32m   4638\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(item)\n\u001b[1;32m-> 4639\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4641\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   4643\u001b[0m     \u001b[38;5;66;03m# for a chain\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\pandas\\core\\frame.py:4010\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   4006\u001b[0m \u001b[38;5;66;03m# icol\u001b[39;00m\n\u001b[0;32m   4007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4008\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[1;32m-> 4010\u001b[0m     col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4011\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_box_col_values(col_mgr, i)\n\u001b[0;32m   4013\u001b[0m     \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1016\u001b[0m, in \u001b[0;36mBlockManager.iget\u001b[1;34m(self, i, track_ref)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miget\u001b[39m(\u001b[38;5;28mself\u001b[39m, i: \u001b[38;5;28mint\u001b[39m, track_ref: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SingleBlockManager:\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;124;03m    Return the data as a SingleBlockManager.\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1016\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblknos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1017\u001b[0m     values \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39miget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblklocs[i])\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;66;03m# shortcut for select a single-dim from a 2-dim BM\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# actually using the methods\n",
    "USGS_stream_flow_all_df\n",
    "MMSD_sewerflow_all_df\n",
    "MMSD_flow_and_precip_all_df\n",
    "MMSD_precip_all_df\n",
    "\n",
    "# Create total precipitation level column to find dry season\n",
    "MMSD_precip_all_df = create_total_col(MMSD_precip_all_df)\n",
    "\n",
    "# Find dry period from precipitation data\n",
    "MMSD_precip_dry_periods = find_consecutive_dry_period(MMSD_precip_all_df, 10, 9, 0.0)\n",
    "\n",
    "# Extract dry season data from sewer flow to get dry season diurnal data\n",
    "MMSD_sewerflow_dry_df = select_dry_period(MMSD_sewerflow_all_df, MMSD_precip_dry_periods)\n",
    "\n",
    "# Get avg from [Sunday 0:00 to Saturday 23:00] of the dry season sewer flow\n",
    "MMSD_sewerflow_dry_dailyavg_df = avg_days_of_week(MMSD_sewerflow_dry_df)\n",
    "\n",
    "# Subtract the average from the full period sewer flow\n",
    "MMSD_sewerflow_all_removed_df = subtract_diurnal(MMSD_sewerflow_all_df, MMSD_sewerflow_dry_dailyavg_df)\n",
    "\n",
    "# Apply 7 passes of Lyne-Hollick\n",
    "MMSD_sewerflow_all_removed_7pass_df = lyne_hollick(insert_mirrored_rows(MMSD_sewerflow_all_removed_df), 7, 0.925)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f35c6b-ef81-4f5d-8a29-4bc8c93f4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# rewrite code for removing diurnal variation \n",
    "# have to use processed precip data instead of day cumulative\n",
    "\n",
    "# for removing diurnal variation\n",
    "# during dry season when theres no rain, is there no base water flow? -> assume diurnal is 0 at min\n",
    "# lyne hollick\n",
    "# maybe need another function since precipitation lingers quite longer -> apply other filter before lh\n",
    "# the shape of orig function is very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a805e62-011e-47ac-872e-aefa91920667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyne_hollick(df, passes=1, alpha=0.925):\n",
    "    \"\"\" \n",
    "    RECURSIVE METHOD\n",
    "    Not to be called manually\n",
    "    Applies Lyne-Hollick filter recursively\n",
    "    \"\"\"\n",
    "    if passes < 1:\n",
    "        return df\n",
    "        \n",
    "    out_df = df.copy()\n",
    "    # On even passes, applies it from the end of df\n",
    "    # Reverse the df \n",
    "    if passes%2==0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "        \n",
    "    columns_to_edit = df.copy().drop(columns='Date Time').columns\n",
    "    for col in columns_to_edit: \n",
    "        out_df[f'{col} new'] = out_df[col] \n",
    "        for i in range(1, len(out_df)):\n",
    "            out_df.at[i, f'{col} new'] = out_df.at[i, col] - np.maximum(\n",
    "                alpha*(out_df.at[i-1,col] - out_df.at[i-1,f'{col} new']) + ((1+alpha)/2)*(out_df.at[i,col] - out_df.at[i-1,col]), 0)\n",
    "            if pd.isna(out_df.at[i, f'{col} new']):\n",
    "                out_df.at[i, f'{col} new'] = out_df.at[i, col]\n",
    "            elif out_df.at[i, f'{col} new'] <= 0: #need to make value dynamic\n",
    "                out_df.at[i, f'{col} new'] = out_df.at[i-1, f'{col} new']\n",
    "\n",
    "    # columns = [f'{col} new' if col in columns_to_rename else col for col in correction_df.columns]\n",
    "    # out_df = out_df[['Date Time'] + [col for col in df.columns if col != 'Date Time']]\n",
    "    print(out_df.dtypes)\n",
    "    new_columns = [col for col in out_df.columns if col.endswith(' new')]\n",
    "    out_df = out_df[['Date Time'] + new_columns]\n",
    "    print(out_df.dtypes)\n",
    "    \n",
    "    out_df.columns = [df.columns]\n",
    "    \n",
    "    # Flips back df on even passes\n",
    "    if passes%2==0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "    # Recursive step\n",
    "    if passes > 1:\n",
    "        return lyne_hollick(out_df, passes-1, alpha)\n",
    "    else:\n",
    "        return out_df\n",
    "\n",
    "        \n",
    "def lyne_hollick_init(df, passes=1, alpha=0.925):\n",
    "    \"\"\" \n",
    "    Use this method when applying Lyne-Hollick\n",
    "    Adds mirrored rows, then applies Lyne-Hollick\n",
    "    \"\"\"\n",
    "    out_df = lyne_hollick((df), passes, alpha)\n",
    "    return out_df.iloc[30:-30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b6130-5d9a-40ce-b703-84733e119b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyne_hollick(df, passes=1, alpha=0.925):\n",
    "    \n",
    "    if passes < 1:\n",
    "        return df\n",
    "    out_df = df.copy()\n",
    "    if passes%2==0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    columns_to_rename = df.copy().drop(columns='Date Time').columns\n",
    "    # correction_df.columns = [col + '_y' if col in columns_to_rename else col for col in correction_df.columns]\n",
    "    for col in columns_to_rename: \n",
    "        out_df[f'{col}_y'] = out_df[col] \n",
    "        for i in range(1, len(out_df)):\n",
    "            out_df.at[i, f'{col}_y'] = out_df.at[i, col] - np.maximum(\n",
    "                alpha*(out_df.at[i-1,col] - out_df.at[i-1,f'{col}_y']) + ((1+alpha)/2)*(out_df.at[i,col] - out_df.at[i-1,col]),\n",
    "                0)\n",
    "            if pd.isna(out_df.at[i, f'{col}_y']):\n",
    "                out_df.at[i, f'{col}_y'] = out_df.at[i, col]\n",
    "            elif out_df.at[i, f'{col}_y'] <= 0: #need to make value dynamic\n",
    "                out_df.at[i, f'{col}_y'] = out_df.at[i-1, f'{col}_y']\n",
    "    out_df = out_df['Date Time', columns_to_rename]\n",
    "    out_df.columns = [df.columns]\n",
    "    if passes%2==0:\n",
    "        out_df = out_df.iloc[::-1].reset_index(drop=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if passes > 1:\n",
    "        return lyne_hollick(out_df, passes-1, alpha)\n",
    "    else:\n",
    "        return out_df.iloc[30:-30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
